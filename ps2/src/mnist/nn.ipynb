{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    e_s = np.sum(e_x, axis=1, keepdims=True)\n",
    "    e = e_x / e_s\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    sigm = 1 / (1 + np.exp(-x))\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_params(input_size, num_hidden, num_output):\n",
    "    parameters = {}\n",
    "    parameters[\"W1\"] = np.random.standard_normal((input_size, num_hidden))\n",
    "    parameters[\"b1\"] = np.zeros((1, num_hidden), dtype=float)\n",
    "    parameters[\"W2\"] = np.random.standard_normal((num_hidden, num_output))\n",
    "    parameters[\"b2\"] = np.zeros((1, num_output), dtype=float)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(data, labels, params):\n",
    "    W1 = params[\"W1\"]\n",
    "    b1 = params[\"b1\"]\n",
    "    W2 = params[\"W2\"]\n",
    "    b2 = params[\"b2\"]\n",
    "\n",
    "    z1 = data @ W1 + b1\n",
    "    a = sigmoid(z1)\n",
    "    z2 = a @ W2 + b2\n",
    "    y = softmax(z2)\n",
    "    cross_entropy = np.multiply(labels, -np.log(y)).sum()\n",
    "    cross_entropy /= y.shape[0]\n",
    "\n",
    "    return a, y, cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(data, labels, params, forward_prop_func):\n",
    "    W1 = params[\"W1\"]\n",
    "    b1 = params[\"b1\"]\n",
    "    W2 = params[\"W2\"]\n",
    "    b2 = params[\"b2\"]\n",
    "    weight = data.shape[0]\n",
    "\n",
    "    a, y, cross_entropy = forward_prop_func\n",
    "    diff_1 = (y - labels)\n",
    "    gradient_W2 = a.T @ diff_1\n",
    "    gradient_b2 = np.sum(diff_1, axis=0, keepdims=True)\n",
    "\n",
    "    diff_2 =  np.multiply((diff_1 @ W2.T), a * (1 - a))\n",
    "    gradient_W1 = data.T @ diff_2\n",
    "    gradient_b1 = np.sum(diff_2, axis=0, keepdims=True)\n",
    "\n",
    "    gradient = {}\n",
    "    gradient[\"W1\"] = gradient_W1 / weight\n",
    "    gradient[\"b1\"] = gradient_b1 / weight\n",
    "    gradient[\"W2\"] = gradient_W2 / weight\n",
    "    gradient[\"b2\"] = gradient_b2 / weight\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-7-64b82c62908b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-64b82c62908b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def backward_prop_regularized(data, labels, params, forward_prop_func, reg):\u001b[0m\n\u001b[0m                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def backward_prop_regularized(data, labels, params, forward_prop_func, reg):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_epoch(train_data, train_labels, learning_rate, batch_size, params, forward_prop_func, backward_prop_func):\n",
    "    params['W1'] -= learning_rate * backward_prop_func['W1']\n",
    "    params['b1'] -= learning_rate * backward_prop_func['b1']\n",
    "    params['W2'] -= learning_rate * backward_prop_func['W2']\n",
    "    params['b2'] -= learning_rate * backward_prop_func['b2']\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train(\n",
    "    train_data, train_labels, dev_data, dev_labels,\n",
    "    get_initial_params_func, forward_prop_func, backward_prop_func,\n",
    "    num_hidden=300, learning_rate=5, num_epochs=30, batch_size=1000):\n",
    "\n",
    "    (nexp, dim) = train_data.shape\n",
    "\n",
    "    params = get_initial_params_func(dim, num_hidden, 10)\n",
    "\n",
    "    cost_train = []\n",
    "    cost_dev = []\n",
    "    accuracy_train = []\n",
    "    accuracy_dev = []\n",
    "    for epoch in range(num_epochs):\n",
    "        gradient_descent_epoch(train_data, train_labels,\n",
    "            learning_rate, batch_size, params, forward_prop_func, backward_prop_func)\n",
    "\n",
    "        h, output, cost = forward_prop_func(train_data, train_labels, params)\n",
    "        cost_train.append(cost)\n",
    "        accuracy_train.append(compute_accuracy(output,train_labels))\n",
    "        h, output, cost = forward_prop_func(dev_data, dev_labels, params)\n",
    "        cost_dev.append(cost)\n",
    "        accuracy_dev.append(compute_accuracy(output, dev_labels))\n",
    "\n",
    "    return params, cost_train, cost_dev, accuracy_train, accuracy_dev\n",
    "\n",
    "def nn_test(data, labels, params):\n",
    "    h, output, cost = forward_prop(data, labels, params)\n",
    "    accuracy = compute_accuracy(output, labels)\n",
    "    return accuracy\n",
    "\n",
    "def compute_accuracy(output, labels):\n",
    "    accuracy = (np.argmax(output,axis=1) ==\n",
    "        np.argmax(labels,axis=1)).sum() * 1. / labels.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "def one_hot_labels(labels):\n",
    "    one_hot_labels = np.zeros((labels.size, 10))\n",
    "    one_hot_labels[np.arange(labels.size),labels.astype(int)] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "def read_data(images_file, labels_file):\n",
    "    x = np.loadtxt(images_file, delimiter=',')\n",
    "    y = np.loadtxt(labels_file, delimiter=',')\n",
    "    return x, y\n",
    "\n",
    "def run_train_test(name, all_data, all_labels, backward_prop_func, num_epochs):\n",
    "    params, cost_train, cost_dev, accuracy_train, accuracy_dev = nn_train(\n",
    "        all_data['train'], all_labels['train'],\n",
    "        all_data['dev'], all_labels['dev'],\n",
    "        get_initial_params, forward_prop, backward_prop_func,\n",
    "        num_hidden=300, learning_rate=5, num_epochs=num_epochs, batch_size=1000\n",
    "    )\n",
    "\n",
    "    t = np.arange(num_epochs)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "    ax1.plot(t, cost_train,'r', label='train')\n",
    "    ax1.plot(t, cost_dev, 'b', label='dev')\n",
    "    ax1.set_xlabel('epochs')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_title('With Regularization')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(t, accuracy_train,'r', label='train')\n",
    "    ax2.plot(t, accuracy_dev, 'b', label='dev')\n",
    "    ax2.set_xlabel('epochs')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.legend()\n",
    "\n",
    "    fig.savefig('./' + name + '.pdf')\n",
    "\n",
    "    accuracy = nn_test(all_data['test'], all_labels['test'], params)\n",
    "    print('For model %s, got accuracy: %f' % (name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--num_epochs'], dest='num_epochs', nargs=None, const=None, default=30, type=<class 'int'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Train a nn model.')\n",
    "parser.add_argument('--num_epochs', type=int, default=30)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "train_data, train_labels = read_data('./images_train.csv', './labels_train.csv')\n",
    "train_labels = one_hot_labels(train_labels)\n",
    "p = np.random.permutation(60000)\n",
    "train_data = train_data[p,:]\n",
    "train_labels = train_labels[p,:]\n",
    "\n",
    "dev_data = train_data[0:10000,:]\n",
    "dev_labels = train_labels[0:10000,:]\n",
    "train_data = train_data[10000:,:]\n",
    "train_labels = train_labels[10000:,:]\n",
    "\n",
    "mean = np.mean(train_data)\n",
    "std = np.std(train_data)\n",
    "train_data = (train_data - mean) / std\n",
    "dev_data = (dev_data - mean) / std\n",
    "\n",
    "test_data, test_labels = read_data('./images_test.csv', './labels_test.csv')\n",
    "test_labels = one_hot_labels(test_labels)\n",
    "test_data = (test_data - mean) / std\n",
    "\n",
    "all_data = {\n",
    "    'train': train_data,\n",
    "    'dev': dev_data,\n",
    "    'test': test_data\n",
    "}\n",
    "\n",
    "all_labels = {\n",
    "    'train': train_labels,\n",
    "    'dev': dev_labels,\n",
    "    'test': test_labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5932acdf91b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'baseline'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "run_train_test('baseline', all_data, all_labels, backward_prop, args.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train_test('baseline', all_data, all_labels, backward_prop, args.num_epochs)\n",
    "run_train_test('regularized', all_data, all_labels,\n",
    "    lambda a, b, c, d: backward_prop_regularized(a, b, c, d, reg=0.0001),\n",
    "    args.num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
